>> pip install scrapy


创建爬虫
1.创建项目 ，‘scrapy startproject [爬虫名字]’
2.创建爬虫：进入到项目所在的路径，执行命令： scrapy gensspider [爬虫名字] [爬虫的域名] 注意，爬虫名字不能和项目名称一致

###项目目录结构
1.item.py:用来存放爬虫爬取下来数据的模型
2.middlewares.py：用来存放各种中间件的文件
3.piplines.py：用来将items的模型存储到本地磁盘中。
4.settings.py：本爬虫的一些配置信息(比如请求头,多久发送一次请求，ip代理池等)
5. scrapy.cfg:项目的配置文件
6.spiders包：以后所有的爬虫，都是存放到这个里面。


###糗事百科爬虫笔记
1.response 是一个'scrapy.http.response.html.Htmlresponse'对象，可以执行xpath和css语法来提取数据
2.提取出来的数据是一个‘Slector’或者是一个‘SelectList’对象，如果想要获取其中的字符串，那么应该执行‘getall’或者‘get’方法
3.getall方法：获取的是'Selector'中的所有文本，返回的是一个列表。
4.get方法：获取的是'Selector'中的第一个文本，返回的是一个str类型。
5.如果数据解析回来，要传给pipline处理，那么可以使用‘yield’来返回，或者是收集所有的item，最后统一return返回
6.item:建议在‘items.py’中定义好模型，以后就不要使用字典。
7.pipline:这个是专门用来保存数据的，其中有三个方法是会经常使用的。
    *‘open_spider(self,spider)’:当爬虫打开时执行。
    *‘process_item(self,item,spider)’:当爬虫有item传过来的时候会被调用。
    *‘close_spider(self,spider)’:当爬虫关闭时会被调用。
    要激活pipline，应该在‘setting.py’中，设置‘ITEM_PIPELINES’.
    示例如下：
    ```python
    ITEM_PIPELINES = {
    'qsbk.pipelines.QsbkPipeline': 300,
}
    ```

### JsonItemExporter 和 JsonLinesItemsExporter:
保存json数据的时候，可以使用这两个类，让操作变得简单。
1.'JsonItemExporter',这个是每次把数据添加到内存中，最后统一写入到磁盘中，好处是，存储的数据是一个满足json规则的数据，坏处是入股偶数据量比较大，那么比较耗内存
示例代码如下：
```python
class QsbkPipeline:
    def open_spider(self,spider):
        #pass
        #也可以在__init__(self):中写
        print("crawling begin...")
        self.fp=open('duanzi.json','wb')#,encoding='utf-8')
        self.exporter = JsonItemExporter(self.fp,ensure_ascii=False,encoding='utf-8')
        self.exporter.start_exporting()
    def process_item(self, item, spider):
        #item_json=json.dumps(dict(item),ensure_ascii=False)
        #self.fp.write(item_json+'\n')
        self.exporter.export_item(item)
        return item
    
    def close_spider(self,spider):
        self.exporter.finish_exporting()
        self.fp.close()
        print("crawling end...")
```
2.'JsonLinesItemsExporter':这个是每次调用'export_item'的时候就把整个item存储到
硬盘中，坏处是每一个字典是一行，整个文件不是一个满足json格式的文件，好处是每次处理数据的时候就直接存储到了硬盘中，这样不会
消耗内存，数据也比较安全。
示例代码如下：
```python
from scrapy.exporters import JsonLinesItemExporter
class QsbkPipeline:
    def open_spider(self,spider):
        #pass
        #也可以在__init__(self):中写
        print("crawling begin...")
        self.fp=open('duanzi.json','wb')#,encoding='utf-8')
        self.exporter = JsonLinesItemExporter(self.fp,ensure_ascii=False,encoding='utf-8')
        #self.exporter.start_exporting()
    def process_item(self, item, spider):
        #item_json=json.dumps(dict(item),ensure_ascii=False)
        #self.fp.write(item_json+'\n')
        self.exporter.export_item(item)
        return item
    
    def close_spider(self,spider):
        #self.exporter.finish_exporting()
        self.fp.close()
        print("crawling end...")
```

scrapy gensspider -c [爬虫名字] [爬虫的域名] #加 -c代表使用的是CrawlSpider爬虫


### CrawlSpider:
需要使用‘LinkExtractor’和‘Rule’.这两个东西决定爬虫的具体走向。
1. allow设置规则的方法：要能够限制在我们想要的url上面。不要跟其他的url产生相同的正则表达式即可。
2. 什么情况下使用follow：如果在爬取页面的时候，需要将满足当前条件的url再进行跟进，那么久设置为True,否则设置为False。
3. 什么情况下该指定callback：如果这个url对应的页面，只是为了获取更多的url，并不需要里面的数据，那么可以不指定callback。如果想要获取url对应页面中的数据，那么就需要指定一个callback。


###Scrapy shell:
1.可以方便我们做一些数据提取的测试代码。
2.如果想要执行scrapy命令，那么毫无疑问，肯定首先要进入到scrapy所在的环境中。
3.若果想要读取某个项目的配置信息，那么应该先进入到这个项目中。再执行‘scrapy shell’，命令。

###模拟登陆人人网：
1.想要发送post请求，那么推荐使用'scrapy.FormRequest'
方法，可以方便指定表单数据。
2.如果想在爬虫一开始的时候就发送post请求，那么应该重写'start_request'
方法。在这个方法中，发送post请求。  


###redis操作：
redis-server.exe redis.windows.conf启动redis服务
redis-cil 启动客户端
redis -h xxx -p【ip】 xxx[端口]

set key value EX timeout
或
setex key timeout value

keys * 查看所有存储的内容

列表插入
lpush [列表名] [数据]
rpush [列表名] [数据]
lrange key start stop #stop取-1表示区所有元素

###移除元素
lpop key
rpop key
lrem [key] [count] [value]
count>0 从表头开始移除count个与value值相等的元素元素
count<0 从表尾开始移除|count|个与value值相等的元素
count=0 移除所有与value值相等的元素

lindex baidu 0定位下标对应的值
llen baidu 元素个数

sadd set value1 value2... ##set 添加元素
For example:sadd team xiaoming xiaohong
 smember set ##查看元素
 for example: smember team

 srem set member
 sream team xiaoming

 scard set ##查看集合中的元素个数。
 如 scrad team

 ##获取多个集合的空集
  sinter set1 set2
  如： sinter team1 team2

  ### 获取多个集合的并集：
  sunion set1 set2
  如： sunion team1 team2

  ##获取多个集合的差集
  sdiff set1 set2

  #hash
  #添加一个
   hset website baidu www.baidu.com

   #获取一个field对应的值
   hget website baidu

   #获取所有的键值对
   hgetall key
   hgetall website

   #获取所有的键
   hkeys key
   hkeys website

   #获取所有的值
   hvals website

   #删除hash里面的一个field
    hdel key field
    如： hdel website field

# 判断哈希中是否存在某个field：
hexists key field
如：hexists website baidu

#判断有多少个键值对
hlen key

##分布式爬虫
1.获取所有的城市的url链接。
2.获取所有城市的新房的url链接
3.获取所有城市的二手房链接。
https://ankang.fang.com/ #获取安康的首页
https://www.fang.com/SoufunFamily.htm来获取所有的新房的链接
例：https://ankang.newhouse.fang.com/house/s/ 来获取安康的新房的链接
https://ankang.esf.fang.com/ 来获取二手房的网页
北京是个例外，北京的新房链接